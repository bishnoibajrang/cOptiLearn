# ðŸ”§ First-Order Optimization Algorithms in C

An efficient and modular C-based library for implementing first-order optimization techniques widely used in machine learning and numerical optimization. From gradient descent variants like Momentum, Nesterov Accelerated Gradient (NAG), Adam, Adagrad, and RMSProp to training models like linear and logistic regression â€” this project builds everything from scratch without external libraries.

ðŸ§  Ideal for educational, research, and performance-critical applications that need full control over optimization logic in pure C.

## ðŸš€ Key Features
- âœ… Scalar and multi-dimensional gradient descent  
- âœ… Adaptive learning algorithms (Adam, Adagrad, RMSProp)  
- âœ… Line search using Armijo rule  
- âœ… Logistic & Linear Regression using all optimizers  
- âœ… Clean modular design using headers and source separation  
- âœ… Easy benchmarking and comparison across optimizers  
- âœ… Works on real datasets (e.g., Iris CSV)  

## ðŸ“‚ Project Structure

```
.
â”œâ”€â”€ include/          # Header files (declarations)
â”œâ”€â”€ src/              # Source files (core logic)
â”œâ”€â”€ examples/         # Usage examples, regressions, optimizer tests
â”œâ”€â”€ data/             # CSV datasets (e.g., iris.csv)
â”œâ”€â”€ Makefile          # Build automation
â””â”€â”€ README.md         # You're here!
```

## ðŸ§± Requirements

- ðŸ–¥ï¸ GCC compiler (or compatible)
- ðŸ”§ Make (optional, for automation)
- ðŸ§¼ No external libraries â€” pure C

## âš™ï¸ Build & Run

### ðŸ”¨ Build All

```bash
make
```

### â–¶ï¸ Run Examples

```bash
make run_regression_logistic
make run_optimizer_adam
make run_gd_scalar_1d
```

Or run manually:

```bash
./run_regression_logistic
```

## ðŸ“Œ Optimizers Included

| Optimizer             | File                         | Description                              |
|-----------------------|------------------------------|------------------------------------------|
| Gradient Descent      | gd_scalar_1d.c, gd_multidim.c| Basic optimization on scalar and vector functions |
| Armijo Line Search    | optimizer_gd_armijo.c        | Adaptive step size with Armijo backtracking |
| Momentum              | optimizer_momentum.c         | Velocity-based accelerated descent       |
| Nesterov (NAG)        | optimizer_nesterov.c         | Look-ahead gradient update               |
| Adagrad               | optimizer_adagrad.c          | Adaptive learning rate per parameter     |
| RMSProp               | optimizer_rmsprop.c          | Smoothed gradient-based learning rate    |
| Adam                  | optimizer_adam.c             | Combines Momentum + RMSProp              |

## ðŸ§® Machine Learning Models

| Model                | File                    | Highlights                                |
|----------------------|-------------------------|--------------------------------------------|
| Linear Regression    | regression_linear.c     | Train with any optimizer                   |
| Logistic Regression  | regression_logistic.c   | Binary classification using sigmoid        |
| Softmax Regression   | regression_softmax.c    | Multiclass classification                  |
| Iris Dataset Classifier | regression_iris.c    | Train/test split with Iris CSV (binary)    |

## ðŸ“Š Example Output

```bash
./run_regression_linear
```

```yaml
--- Momentum Optimizer ---
Iter 1000 | Loss: 0.00023
Weights: [0.9987, 1.0021]

--- Adam Optimizer ---
Iter 1000 | Loss: 0.00012
Weights: [1.0001, 0.9998]
```

## ðŸ§ª Real Dataset Support

- Load CSV files like iris.csv for model training and testing.
- Supports classification labels like "Setosa", "Versicolor"
- Includes train-test split and normalization
- Accurate label parsing and memory management

## ðŸ§­ Roadmap

- [x] First-order optimizers in modular C
- [x] Linear & logistic regression from scratch
- [x] Softmax classifier with real CSV data
- [ ] Batch & stochastic gradient variants
- [ ] Export results as CSV
- [ ] Integration with plotting tools (Python / Gnuplot)
- [ ] Second-order optimizers (Newton, BFGS, etc.)

## ðŸ‘¤ Author

**Bishnoi Bajrang**  
Passionate about low-level performance, numerical optimization, and building machine learning from the ground up.

ðŸ”— GitHub Â Â ðŸ“§ bishnoibajrang502@gmail.com

## ðŸ“œ License

This project is licensed under the MIT License â€” free to use, modify, and distribute.

## ðŸ¤ Contributions Welcome!

PRs, issues, and forks are welcome. Whether it's adding a new optimizer or improving existing logic â€” contributions make this project better.

## ðŸ·ï¸ SEO Tags

*C Gradient Descent Â· Momentum in C Â· Optimization Algorithms in C Â· Machine Learning in C Â· Adam Optimizer in C Â· Logistic Regression Pure C Â· RMSProp Implementation C Â· Armijo Line Search C Â· Iris Dataset C Project Â· ML from Scratch C*
=======
# cOptiLearn
Optimization Algorithms Implemented from Scratch in C
>>>>>>> e4870c443bbdc6bbfeb8d6f7224fd60475bb8367
